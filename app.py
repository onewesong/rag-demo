import streamlit as st
import openai
import dotenv
import os
import chromadb
from markitdown import MarkItDown
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import uuid

dotenv.load_dotenv()
md = MarkItDown()

chroma_client = chromadb.Client()

# åˆå§‹åŒ–æˆ–è·å– collection
st.session_state.collection = chroma_client.get_or_create_collection(
    name="document_chunks",
    metadata={"description": "æ–‡æ¡£åˆ‡ç‰‡é›†åˆ"}
)

if 'handled_files' not in st.session_state:
    st.session_state.handled_files = []

# åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªåˆ‡ç‰‡çš„å­—ç¬¦æ•°
    chunk_overlap=200,  # åˆ‡ç‰‡ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
)

# é¡µé¢æ ‡é¢˜
st.header("ğŸ’¬ RAG Demo", divider="rainbow")
st.caption("ğŸš€ åŸºäº Streamlitã€Chroma å’Œå¤§æ¨¡å‹ API çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

@st.cache_data
def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    models = []
    try:
        for model in llm_client.models.list().data:
            if 'rerank' in model.id.lower() or 'bge' in model.id.lower():
                continue
            models.append(model.id)
    except Exception as e:
        st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []
    return models

def process_and_store_document(file):
    """å¤„ç†å¹¶å­˜å‚¨æ–‡æ¡£"""
    try:
        # å°†æ–‡æ¡£è½¬æ¢ä¸º Markdown
        content = md.convert(file).text_content
        
        # åˆ‡åˆ†æ–‡æ¡£
        chunks = text_splitter.split_text(content)
        
        # ç”Ÿæˆæ–‡æ¡£IDï¼ˆåŸºäºæ–‡ä»¶åçš„å“ˆå¸Œï¼‰
        file_hash = hashlib.md5(file.name.encode()).hexdigest()
        
        # å­˜å‚¨æ¯ä¸ªåˆ‡ç‰‡
        chunk_ids = []
        chunk_docs = []
        chunk_metas = []
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{file_hash}_{i}"
            chunk_ids.append(chunk_id)
            chunk_docs.append(chunk)
            chunk_metas.append({
                "source": file.name,
                "chunk_index": i,
                "total_chunks": len(chunks)
            })
        
        # æ‰¹é‡æ·»åŠ åˆ° Chroma
        st.session_state.collection.add(
            documents=chunk_docs,
            metadatas=chunk_metas,
            ids=chunk_ids,
        )
        
        return True, len(chunks), content
    except Exception as e:
        return False, 0, str(e)

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.subheader("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    # API é…ç½®
    base_url = st.text_input("OpenAI Base URL", value=os.getenv("OPENAI_BASE_URL", ""))
    api_key = st.text_input(
        "OpenAI API Key", 
        value=os.getenv("OPENAI_API_KEY", ""), 
        type="password", 
        help="è·å– API Key: [ç™½å±±å¤§æ¨¡å‹](https://ai.baishan.com/auth/login?referralCode=ttXv0P1zRH)"
    )
    
    if not api_key:
        st.error("âš ï¸ è¯·è¾“å…¥ OpenAI API Key")
        st.stop()
    
    llm_client = openai.OpenAI(base_url=base_url, api_key=api_key)
    
    # æ¨¡å‹é€‰æ‹©
    model = st.selectbox("é€‰æ‹©æ¨¡å‹", get_models())
    
    # æ£€ç´¢å‚æ•°é…ç½®
    st.divider()
    st.subheader("ğŸ” æ£€ç´¢å‚æ•°")
    n_results = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", min_value=1, max_value=10, value=3)
    
    # æ–‡æ¡£ä¸Šä¼ 
    st.divider()
    st.subheader("ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£", 
        type=["pdf", "docx", "txt", "md", "pptx", "xlsx"],
        accept_multiple_files=True,
        help="æ”¯æŒ PDFã€Wordã€æ–‡æœ¬ã€Markdownã€PPTã€Excel ç­‰æ ¼å¼"
    )
    if uploaded_files:
        for file in uploaded_files:
            if file.name in st.session_state.handled_files:
                continue
            st.session_state.handled_files.append(file.name)
            with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}..."):
                success, chunks_count, result = process_and_store_document(file)
                
                if success:
                    st.success(f"âœ… {file.name} å·²å¤„ç†ï¼ˆå…± {chunks_count} ä¸ªåˆ‡ç‰‡ï¼‰")
                    with st.expander(f"ğŸ“„ æŸ¥çœ‹ {file.name} å†…å®¹"):
                        st.text(result[:1000] + "..." if len(result) > 1000 else result)
                else:
                    st.error(f"âŒ {file.name} å¤„ç†å¤±è´¥: {result}")
    
    # æ•°æ®åº“ç»Ÿè®¡
    st.divider()
    st.subheader("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    try:
        collection_count = st.session_state.collection.count()
        st.metric("æ–‡æ¡£åˆ‡ç‰‡æ•°", collection_count)
    except:
        st.metric("æ–‡æ¡£åˆ‡ç‰‡æ•°", "N/A")
    
    # æ¸…ç©ºæ•°æ®åº“
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®åº“"):
        chroma_client.delete_collection("document_chunks")
        st.session_state.handled_files = []
        st.toast("æ•°æ®åº“å·²æ¸…ç©º")
        st.rerun()
    # èŠå¤©å¤„ç†
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
    # è°ƒè¯•é€‰é¡¹
    st.divider()
    if st.toggle("ğŸ› æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"):
        st.write("èŠå¤©å†å²ï¼š", st.session_state.get('messages', []))

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        # å¦‚æœæœ‰æ£€ç´¢ç»“æœï¼Œæ˜¾ç¤ºæ¥æº
        if "sources" in message:
            with st.expander("ğŸ“š å‚è€ƒæ¥æº"):
                for i, source in enumerate(message["sources"], 1):
                    st.markdown(f"**æ¥æº {i}:** {source['metadata']['source']} (åˆ‡ç‰‡ {source['metadata']['chunk_index']+1}/{source['metadata']['total_chunks']})")
                    st.text(source['document'][:200] + "..." if len(source['document']) > 200 else source['document'])

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        results = st.session_state.collection.query(
            query_texts=[prompt],
            n_results=n_results,
        )
        
        # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        retrieved_docs = []
        if results['documents'] and len(results['documents']) > 0:
            for i in range(len(results['documents'][0])):
                retrieved_docs.append({
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                })
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc['document'] for doc in retrieved_docs])
        
        # æ„å»ºæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š
<context>
{context}
</context>

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ç”¨æˆ·ã€‚"""

    # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    with st.spinner("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
        
        response = llm_client.chat.completions.create(
            model=model, 
            messages=messages,
            temperature=0.7,
        )
        
        answer = response.choices[0].message.content
        
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
        st.session_state.messages.append({
            "role": "assistant", 
            "content": answer,
            "sources": retrieved_docs
        })
        
        # æ˜¾ç¤ºç­”æ¡ˆ
        with st.chat_message("assistant"):
            st.write(answer)
            
            # æ˜¾ç¤ºå‚è€ƒæ¥æº
            if retrieved_docs:
                for i, doc in enumerate(retrieved_docs, 1):
                    with st.expander(f"**æ¥æº {i}:** {doc['metadata']['source']} (åˆ‡ç‰‡ {doc['metadata']['chunk_index']+1}/{doc['metadata']['total_chunks']})"):
                        if doc['distance'] is not None:
                            st.caption(f"ç›¸ä¼¼åº¦è·ç¦»: {doc['distance']:.4f}")
                        st.text(doc['document'][:200] + "..." if len(doc['document']) > 200 else doc['document'])
                        st.divider()
        
        
