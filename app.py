import streamlit as st
import openai
import dotenv
import os
import chromadb
from markitdown import MarkItDown
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import uuid

dotenv.load_dotenv()
md = MarkItDown()

chroma_client = chromadb.Client()

# åˆå§‹åŒ–æˆ–è·å– collection
st.session_state.collection = chroma_client.get_or_create_collection(
    name="document_chunks",
    metadata={"description": "æ–‡æ¡£åˆ‡ç‰‡é›†åˆ"}
)

if 'handled_files' not in st.session_state:
    st.session_state.handled_files = []

# é¡µé¢æ ‡é¢˜
st.header("ğŸ’¬ RAG Demo", divider="rainbow")
st.caption("ğŸš€ åŸºäº Streamlitã€Chroma å’Œå¤§æ¨¡å‹ API çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

@st.cache_data
def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    models = []
    try:
        for model in llm_client.models.list().data:
            if 'rerank' in model.id.lower() or 'bge' in model.id.lower():
                continue
            models.append(model.id)
    except Exception as e:
        st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []
    return models

def process_and_store_document(file):
    """å¤„ç†å¹¶å­˜å‚¨æ–‡æ¡£"""
    try:
        # å°†æ–‡æ¡£è½¬æ¢ä¸º Markdown
        content = md.convert(file).text_content
        
        # åˆ‡åˆ†æ–‡æ¡£
        chunks = text_splitter.split_text(content)
        
        # ç”Ÿæˆæ–‡æ¡£IDï¼ˆåŸºäºæ–‡ä»¶åçš„å“ˆå¸Œï¼‰
        file_hash = hashlib.md5(file.name.encode()).hexdigest()
        
        # å­˜å‚¨æ¯ä¸ªåˆ‡ç‰‡
        chunk_ids = []
        chunk_docs = []
        chunk_metas = []
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{file_hash}_{i}"
            chunk_ids.append(chunk_id)
            chunk_docs.append(chunk)
            chunk_metas.append({
                "source": file.name,
                "chunk_index": i,
                "total_chunks": len(chunks)
            })
        
        # æ‰¹é‡æ·»åŠ åˆ° Chroma
        st.session_state.collection.add(
            documents=chunk_docs,
            metadatas=chunk_metas,
            ids=chunk_ids,
        )
        
        return True, len(chunks), content
    except Exception as e:
        return False, 0, str(e)

@st.dialog("ğŸ“š æ•°æ®åº“æ–‡æ¡£åˆ‡ç‰‡", width="large")
def show_chunks_dialog():
    """æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£åˆ‡ç‰‡"""
    try:
        collection_count = st.session_state.collection.count()
        
        if collection_count == 0:
            st.info("æ•°æ®åº“ä¸­æš‚æ— æ–‡æ¡£åˆ‡ç‰‡")
            return
        
        # è·å–æ‰€æœ‰æ–‡æ¡£åˆ‡ç‰‡
        all_data = st.session_state.collection.get()
        
        st.write(f"å…±æœ‰ **{collection_count}** ä¸ªæ–‡æ¡£åˆ‡ç‰‡")
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„æ˜¾ç¤º
        docs_by_source = {}
        for i, metadata in enumerate(all_data['metadatas']):
            source = metadata['source']
            if source not in docs_by_source:
                docs_by_source[source] = []
            docs_by_source[source].append({
                'id': all_data['ids'][i],
                'document': all_data['documents'][i],
                'metadata': metadata
            })
        
        # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„åˆ‡ç‰‡
        for source, chunks in docs_by_source.items():
            with st.expander(f"ğŸ“„ {source} ({len(chunks)} ä¸ªåˆ‡ç‰‡)", expanded=True):
                sorted_chunks = sorted(chunks, key=lambda x: x['metadata']['chunk_index'])
                
                # å°†åˆ‡ç‰‡åˆ†æˆä¸¤åˆ—æ˜¾ç¤º
                for i in range(0, len(sorted_chunks), 2):
                    col1, col2 = st.columns(2)
                    
                    # å·¦åˆ—
                    with col1:
                        chunk = sorted_chunks[i]
                        st.markdown(f"**åˆ‡ç‰‡ {chunk['metadata']['chunk_index'] + 1}/{chunk['metadata']['total_chunks']}**")
                        st.text_area(
                            f"ID: {chunk['id']}",
                            chunk['document'],
                            height=200,
                            key=chunk['id'],
                            disabled=True
                        )
                    
                    # å³åˆ—ï¼ˆå¦‚æœè¿˜æœ‰åˆ‡ç‰‡ï¼‰
                    with col2:
                        if i + 1 < len(sorted_chunks):
                            chunk = sorted_chunks[i + 1]
                            st.markdown(f"**åˆ‡ç‰‡ {chunk['metadata']['chunk_index'] + 1}/{chunk['metadata']['total_chunks']}**")
                            st.text_area(
                                f"ID: {chunk['id']}",
                                chunk['document'],
                                height=200,
                                key=chunk['id'],
                                disabled=True
                            )
                
                if len(sorted_chunks) > 0:
                    st.divider()
    
    except Exception as e:
        st.error(f"è·å–æ–‡æ¡£åˆ‡ç‰‡å¤±è´¥: {e}")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.subheader("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    # API é…ç½®
    base_url = st.text_input("OpenAI Base URL", value=os.getenv("OPENAI_BASE_URL", ""))
    api_key = st.text_input(
        "OpenAI API Key", 
        value=os.getenv("OPENAI_API_KEY", ""), 
        type="password", 
        help="è·å– API Key: [ç™½å±±å¤§æ¨¡å‹](https://ai.baishan.com/auth/login?referralCode=ttXv0P1zRH)"
    )
    
    if not api_key:
        st.error("âš ï¸ è¯·è¾“å…¥ OpenAI API Key")
        st.stop()
    
    llm_client = openai.OpenAI(base_url=base_url, api_key=api_key)
    
    # æ¨¡å‹é€‰æ‹©
    model = st.selectbox("é€‰æ‹©æ¨¡å‹", get_models())
    
    # æ£€ç´¢å‚æ•°é…ç½®
    st.divider()
    st.subheader("ğŸ” æ£€ç´¢å‚æ•°")
    n_results = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", min_value=1, max_value=10, value=3)
    
    # æ–‡æ¡£åˆ‡ç‰‡å‚æ•°é…ç½®
    st.divider()
    st.subheader("âœ‚ï¸ æ–‡æ¡£åˆ‡ç‰‡å‚æ•°")
    chunk_size = st.slider(
        "åˆ‡ç‰‡å¤§å°", 
        min_value=100, 
        max_value=2000, 
        value=1000, 
        step=100,
        help="æ¯ä¸ªåˆ‡ç‰‡çš„å­—ç¬¦æ•°"
    )
    chunk_overlap = st.slider(
        "åˆ‡ç‰‡é‡å ", 
        min_value=0, 
        max_value=500, 
        value=200, 
        step=50,
        help="åˆ‡ç‰‡ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"
    )
    
    # åˆ†éš”ç¬¦é…ç½®
    with st.expander("ğŸ”§ é«˜çº§è®¾ç½® - åˆ†éš”ç¬¦é…ç½®"):
        separators_input = st.text_area(
            "åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            value="\\n\\n\n\\n\nã€‚\nï¼\nï¼Ÿ\n.\n!\n?\n \n",
            height=150,
            help="æ–‡æœ¬åˆ‡ç‰‡æ—¶ä½¿ç”¨çš„åˆ†éš”ç¬¦ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½æ’åˆ—ã€‚æ”¯æŒè½¬ä¹‰å­—ç¬¦ï¼Œå¦‚ \\n è¡¨ç¤ºæ¢è¡Œ"
        )
    
    separators = []
    for line in separators_input.strip().split('\n'):
        if line:
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            sep = line.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
            separators.append(sep)
    
    # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not separators:
        separators = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]

    # åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=separators,
    )

    # æ–‡æ¡£ä¸Šä¼ 
    st.divider()
    st.subheader("ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£", 
        type=["pdf", "docx", "txt", "md", "pptx", "xlsx"],
        accept_multiple_files=True,
        help="æ”¯æŒ PDFã€Wordã€æ–‡æœ¬ã€Markdownã€PPTã€Excel ç­‰æ ¼å¼"
    )
    if uploaded_files:
        for file in uploaded_files:
            if file.name in st.session_state.handled_files:
                continue
            st.session_state.handled_files.append(file.name)
            with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}..."):
                success, chunks_count, result = process_and_store_document(file)
                
                if success:
                    st.success(f"âœ… {file.name} å·²å¤„ç†ï¼ˆå…± {chunks_count} ä¸ªåˆ‡ç‰‡ï¼‰")
                    with st.expander(f"ğŸ“„ æŸ¥çœ‹ {file.name} å†…å®¹"):
                        st.text(result[:1000] + "..." if len(result) > 1000 else result)
                else:
                    st.error(f"âŒ {file.name} å¤„ç†å¤±è´¥: {result}")
    
    # æ•°æ®åº“ç»Ÿè®¡
    st.divider()
    st.subheader("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    try:
        collection_count = st.session_state.collection.count()
        st.metric("æ–‡æ¡£åˆ‡ç‰‡æ•°", collection_count)
    except:
        st.metric("æ–‡æ¡£åˆ‡ç‰‡æ•°", "N/A")
    
    # æŸ¥çœ‹æ–‡æ¡£åˆ‡ç‰‡æŒ‰é’®
    if st.button("ğŸ‘€ æŸ¥çœ‹æ–‡æ¡£åˆ‡ç‰‡"):
        show_chunks_dialog()
    
    # æ¸…ç©ºæ•°æ®åº“
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®åº“"):
        chroma_client.delete_collection("document_chunks")
        st.session_state.handled_files = []
        st.toast("æ•°æ®åº“å·²æ¸…ç©º")
        st.rerun()
    # èŠå¤©å¤„ç†
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
    # è°ƒè¯•é€‰é¡¹
    st.divider()
    if st.toggle("ğŸ› æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"):
        st.write("**èŠå¤©å†å²ï¼š**", st.session_state.get('messages', []))
        st.write("**åˆ‡ç‰‡é…ç½®ï¼š**")
        st.json({
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "separators_count": len(separators),
            "separators": [repr(s) for s in separators]
        })

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        # å¦‚æœæœ‰æ£€ç´¢ç»“æœï¼Œæ˜¾ç¤ºæ¥æº
        if "sources" in message:
            with st.expander("ğŸ“š å‚è€ƒæ¥æº"):
                for i, source in enumerate(message["sources"], 1):
                    st.markdown(f"**æ¥æº {i}:** {source['metadata']['source']} (åˆ‡ç‰‡ {source['metadata']['chunk_index']+1}/{source['metadata']['total_chunks']})")
                    st.text(source['document'][:200] + "..." if len(source['document']) > 200 else source['document'])

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        results = st.session_state.collection.query(
            query_texts=[prompt],
            n_results=n_results,
        )
        
        # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        retrieved_docs = []
        if results['documents'] and len(results['documents']) > 0:
            for i in range(len(results['documents'][0])):
                retrieved_docs.append({
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                })
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc['document'] for doc in retrieved_docs])
        
        # æ„å»ºæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š
<context>
{context}
</context>

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ç”¨æˆ·ã€‚"""

    # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]
    
    # æ˜¾ç¤ºç­”æ¡ˆï¼ˆæµå¼ï¼‰
    with st.chat_message("assistant"):
        # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
        message_placeholder = st.empty()
        full_response = ""
        
        # è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
        stream = llm_client.chat.completions.create(
            model=model, 
            messages=messages,
            temperature=0.7,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # é€æ­¥æ¥æ”¶å¹¶æ˜¾ç¤ºå“åº”
        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
        
        # æ˜¾ç¤ºå®Œæ•´å“åº”ï¼ˆç§»é™¤å…‰æ ‡ï¼‰
        message_placeholder.markdown(full_response)
        
        # æ˜¾ç¤ºå‚è€ƒæ¥æº
        if retrieved_docs:
            for i, doc in enumerate(retrieved_docs, 1):
                with st.expander(f"**æ¥æº {i}:** {doc['metadata']['source']} (åˆ‡ç‰‡ {doc['metadata']['chunk_index']+1}/{doc['metadata']['total_chunks']})"):
                    if doc['distance'] is not None:
                        st.caption(f"ç›¸ä¼¼åº¦è·ç¦»: {doc['distance']:.4f}")
                    st.text(doc['document'])
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messages.append({
        "role": "assistant", 
        "content": full_response,
        "sources": retrieved_docs
    })
        
        
