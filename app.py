import streamlit as st
import openai
import dotenv
import os

dotenv.load_dotenv()

if 'handled_files' not in st.session_state:
    st.session_state.handled_files = []

# é¡µé¢æ ‡é¢˜
st.header("ğŸ’¬ RAG Demo", divider="rainbow")
st.caption("ğŸš€ åŸºäº Streamlitã€OpenAI å‘é‡åº“ å’Œå¤§æ¨¡å‹ API çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

@st.cache_data
def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    models = []
    try:
        for model in llm_client.models.list().data:
            if 'rerank' in model.id.lower() or 'bge' in model.id.lower():
                continue
            models.append(model.id)
    except Exception as e:
        st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []
    return models

def process_and_store_document(file):
    """ä¸Šä¼ æ–‡ä»¶åˆ° OpenAI å‘é‡åº“å¹¶ç­‰å¾…ç´¢å¼•å®Œæˆã€‚"""
    try:
        # åˆ›å»ºæˆ–è·å–å‘é‡åº“
        if "vector_store_id" not in st.session_state or not st.session_state.vector_store_id:
            vs = llm_client.vector_stores.create(name="document_store")
            st.session_state.vector_store_id = vs.id

        # ä¸Šä¼ å¹¶ç´¢å¼•åˆ°å‘é‡åº“
        llm_client.vector_stores.files.upload_and_poll(
            vector_store_id=st.session_state.vector_store_id,
            file=file,
        )

        return True, None
    except Exception as e:
        return False, str(e)

@st.dialog("ğŸ“š å‘é‡åº“æ–‡ä»¶ä¸å†…å®¹", width="large")
def show_chunks_dialog():
    """æ˜¾ç¤ºå‘é‡åº“ä¸­çš„æ–‡ä»¶åŠè§£æå†…å®¹ï¼ˆæŒ‰æ–‡ä»¶å±•ç¤ºï¼‰ã€‚"""
    try:
        if "vector_store_id" not in st.session_state or not st.session_state.vector_store_id:
            st.info("å‘é‡åº“ä¸ºç©ºï¼Œå°šæœªä¸Šä¼ ä»»ä½•æ–‡ä»¶ã€‚")
            return

        # åˆ—å‡ºå‘é‡åº“æ–‡ä»¶
        files_page = llm_client.vector_stores.files.list(vector_store_id=st.session_state.vector_store_id, limit=100)
        files_list = getattr(files_page, "data", files_page)

        if not files_list:
            st.info("å‘é‡åº“ä¸­æš‚æ— æ–‡ä»¶")
            return

        st.write(f"å…±æœ‰ **{len(files_list)}** ä¸ªæ–‡ä»¶")

        for f in files_list:
            with st.expander(f"ğŸ“„ {getattr(f, 'filename', getattr(f, 'id', 'file'))} - çŠ¶æ€: {getattr(f, 'status', 'unknown')}"):
                try:
                    contents_page = llm_client.vector_stores.files.content(
                        file_id=f.id,
                        vector_store_id=st.session_state.vector_store_id,
                    )
                    contents = getattr(contents_page, "data", contents_page)
                    # æ¯ä¸ªå†…å®¹é¡¹åŒ…å« text
                    for idx, item in enumerate(contents[:10]):  # ä»…å±•ç¤ºå‰ 10 æ®µï¼Œé¿å…è¿‡é•¿
                        st.text_area(
                            f"å†…å®¹ç‰‡æ®µ {idx+1}",
                            getattr(item, "text", ""),
                            height=200,
                            key=f"{f.id}_content_{idx}",
                            disabled=True,
                        )
                except Exception as ex:
                    st.warning(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {ex}")

    except Exception as e:
        st.error(f"è·å–å‘é‡åº“æ–‡ä»¶å¤±è´¥: {e}")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.subheader("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    # API é…ç½®
    base_url = st.text_input("OpenAI Base URL", value=os.getenv("OPENAI_BASE_URL", "https://api.edgefn.net/v1/"))
    api_key = st.text_input(
        "OpenAI API Key", 
        value=os.getenv("OPENAI_API_KEY", ""), 
        type="password", 
        help="å¯ç‚¹å‡»æ­¤é“¾æ¥è·å– API Key: [ç™½å±±å¤§æ¨¡å‹](https://ai.baishan.com/auth/login?referralCode=ttXv0P1zRH), æ³¨å†Œå³é€150å…ƒ"
    )
    
    if not api_key:
        st.error("âš ï¸ è¯·è¾“å…¥ OpenAI API Key")
        st.info("å¯ç‚¹å‡»æ­¤é“¾æ¥è·å– API Key: [ç™½å±±å¤§æ¨¡å‹](https://ai.baishan.com/auth/login?referralCode=ttXv0P1zRH), æ³¨å†Œå³é€150å…ƒ")
        st.stop()
    
    llm_client = openai.OpenAI(base_url=base_url, api_key=api_key)
    
    # å‘é‡åº“é€‰æ‹©/åˆ›å»º
    st.divider()
    st.subheader("ğŸ—„ï¸ å‘é‡åº“")
    try:
        vs_page = llm_client.vector_stores.list(limit=50)
        vs_list = getattr(vs_page, "data", vs_page)
        vs_options = [f"{(vs.name or vs.id)} ({vs.id})" for vs in vs_list]
        if vs_options:
            selected = st.selectbox(
                "é€‰æ‹©å‘é‡åº“",
                options=list(range(len(vs_options))),
                format_func=lambda i: vs_options[i],
            )
            if selected is not None:
                st.session_state.vector_store_id = vs_list[selected].id
        new_vs_name = st.text_input("æ–°å‘é‡åº“åç§°", value="")
        if st.button("â• åˆ›å»ºå‘é‡åº“"):
            created_vs = llm_client.vector_stores.create(name=new_vs_name or "document_store")
            st.session_state.vector_store_id = created_vs.id
            st.toast("å·²åˆ›å»ºå‘é‡åº“")
            st.rerun()
    except Exception as e:
        st.warning(f"åŠ è½½å‘é‡åº“åˆ—è¡¨å¤±è´¥: {e}")
    
    # æ¨¡å‹é€‰æ‹©
    model = st.selectbox("é€‰æ‹©æ¨¡å‹", get_models())
    
    # æ£€ç´¢å‚æ•°é…ç½®
    st.divider()
    st.subheader("ğŸ” æ£€ç´¢å‚æ•°")
    n_results = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", min_value=1, max_value=20, value=5)
    
    #ï¼ˆå·²ç§»é™¤æœ¬åœ°åˆ‡ç‰‡é…ç½®ï¼Œæ”¹ä¸ºæœåŠ¡ç«¯è‡ªåŠ¨è§£æä¸åˆ‡ç‰‡ï¼‰

    # æ–‡æ¡£ä¸Šä¼ 
    st.divider()
    st.subheader("ğŸ“ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£", 
        type=["pdf", "docx", "txt", "md", "pptx", "xlsx"],
        accept_multiple_files=True,
        help="æ”¯æŒ PDFã€Wordã€æ–‡æœ¬ã€Markdownã€PPTã€Excel ç­‰æ ¼å¼"
    )
    if uploaded_files:
        for file in uploaded_files:
            if file.name in st.session_state.handled_files:
                continue
            st.session_state.handled_files.append(file.name)
            with st.spinner(f"æ­£åœ¨å¤„ç† {file.name}..."):
                success, error = process_and_store_document(file)

                if success:
                    st.success(f"âœ… {file.name} å·²ä¸Šä¼ å¹¶ç´¢å¼•è‡³ OpenAI å‘é‡åº“")
                else:
                    st.error(f"âŒ {file.name} ä¸Šä¼ /ç´¢å¼•å¤±è´¥: {error}")
    
    # å‘é‡åº“ç»Ÿè®¡
    st.divider()
    st.subheader("ğŸ“Š å‘é‡åº“ç»Ÿè®¡")
    try:
        if "vector_store_id" in st.session_state and st.session_state.vector_store_id:
            files_page = llm_client.vector_stores.files.list(vector_store_id=st.session_state.vector_store_id, limit=100)
            files_list = getattr(files_page, "data", files_page)
            st.metric("å‘é‡åº“æ–‡ä»¶æ•°", len(files_list))
        else:
            st.metric("å‘é‡åº“æ–‡ä»¶æ•°", 0)
    except Exception:
        st.metric("å‘é‡åº“æ–‡ä»¶æ•°", "N/A")
    
    # æŸ¥çœ‹æ–‡ä»¶ä¸å†…å®¹æŒ‰é’®
    if st.button("ğŸ‘€ æŸ¥çœ‹å‘é‡åº“å†…å®¹"):
        show_chunks_dialog()
    
    # æ¸…ç©ºå‘é‡åº“
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡åº“"):
        try:
            if "vector_store_id" in st.session_state and st.session_state.vector_store_id:
                llm_client.vector_stores.delete(st.session_state.vector_store_id)
            st.session_state.vector_store_id = None
            st.session_state.handled_files = []
            st.toast("å‘é‡åº“å·²æ¸…ç©º")
            st.rerun()
        except Exception as e:
            st.error(f"æ¸…ç©ºå‘é‡åº“å¤±è´¥: {e}")
    # èŠå¤©å¤„ç†
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
    # è°ƒè¯•é€‰é¡¹
    st.divider()
    if st.toggle("ğŸ› æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"):
        st.write("**èŠå¤©å†å²ï¼š**", st.session_state.get('messages', []))

# æ˜¾ç¤ºå‚è€ƒæ¥æº
def display_retrieved_docs(retrieved_docs):
    if not retrieved_docs:
        return
    for i, doc in enumerate(retrieved_docs, 1):
        source = doc.get('metadata', {}).get('source') or doc.get('metadata', {}).get('filename') or "æœªçŸ¥æ¥æº"
        header = f"**æ¥æº {i}:** {source}"
        with st.expander(header):
            score = doc.get('score')
            distance = doc.get('distance')
            if score is not None:
                st.caption(f"ç›¸ä¼¼åº¦å¾—åˆ†: {score:.4f}")
            elif distance is not None:
                st.caption(f"ç›¸ä¼¼åº¦è·ç¦»: {distance:.4f}")
            st.text(doc['document'])

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "sources" in message:
            display_retrieved_docs(message["sources"])


# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ OpenAI å‘é‡åº“æœç´¢ï¼‰
    with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        retrieved_docs = []
        if "vector_store_id" in st.session_state and st.session_state.vector_store_id:
            try:
                search_page = llm_client.vector_stores.search(
                    st.session_state.vector_store_id,
                    query=prompt,
                    max_num_results=n_results,
                )
                search_results = getattr(search_page, "data", search_page)
                for item in search_results:
                    # item.content æ˜¯åˆ—è¡¨ï¼Œæ‹¼æ¥ä¸ºæ–‡æœ¬
                    texts = []
                    try:
                        for c in getattr(item, "content", [])[:3]:
                            t = getattr(c, "text", None)
                            if t:
                                texts.append(t)
                    except Exception:
                        pass
                    doc_text = "\n\n".join(texts) if texts else ""
                    if not doc_text:
                        # å…œåº•ï¼šè‹¥æ—  contentï¼Œåˆ™è·³è¿‡
                        continue
                    retrieved_docs.append({
                        'document': doc_text,
                        'metadata': {
                            'source': getattr(item, 'filename', 'unknown'),
                        },
                        'score': getattr(item, 'score', None),
                    })
            except Exception as e:
                st.warning(f"å‘é‡åº“æœç´¢å¤±è´¥ï¼Œæ”¹ä¸ºæ— æ£€ç´¢å›ç­”ï¼š{e}")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc['document'] for doc in retrieved_docs])
        
        # æ„å»ºæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š
<context>
{context}
</context>

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘ŠçŸ¥ç”¨æˆ·ã€‚"""

    # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]
    
    # æ˜¾ç¤ºç­”æ¡ˆï¼ˆæµå¼ï¼‰
    with st.chat_message("assistant"):
        # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
        message_placeholder = st.empty()
        full_response = ""
        
        # è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
        stream = llm_client.chat.completions.create(
            model=model, 
            messages=messages,
            temperature=0.7,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # é€æ­¥æ¥æ”¶å¹¶æ˜¾ç¤ºå“åº”
        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
        
        # æ˜¾ç¤ºå®Œæ•´å“åº”ï¼ˆç§»é™¤å…‰æ ‡ï¼‰
        message_placeholder.markdown(full_response)
        
        display_retrieved_docs(retrieved_docs)
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messages.append({
        "role": "assistant", 
        "content": full_response,
        "sources": retrieved_docs
    })
        
        
